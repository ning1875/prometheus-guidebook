# 大纲
- 0.1 开篇词 从问题出发的k8s新源码解读课程逻辑

- 第1章 准备k8s1.29和go源码环境
  - 1.1 都需要准备哪些环境
  - 1.2 先来看k8s的最新版本
  - 1.3 准备ubuntu22.04的虚拟机模板
  - 1.4 准备k8s-1.29仓库源
  - 1.5 手动安装k8s-1.29-master
  - 1.6 整理k8s-1.29-master安装脚本
  - 1.7 整理k8s-1.29-node安装脚本
  - 1.8 安装golang最新版
  - 1.9 下载k8s源码

- 第2章 关于k8s节点宕机
  - 2.1 提出问题
  - 2.2 熟悉一下节点资源
  - 2.3 做实验看下4个具体的判断条件
  - 2.4 NodeDiskPressure设置逻辑
  - 2.5 追踪磁盘Condition上层调用
  - 2.6 kubelet中的Condition判定
  - 2.7 节点压力驱逐和垃圾回收
  - 2.8 熟悉rbac模型
  - 2.9 编写只读rbac和124-secret生成问题
  - 2.10 用token调用k8s各种coreApi

- 第3章 涉及到的apiserver的鉴权
  - 3.1 调用其他group的api
  - 3.2 查看configz的配置回答2.7的问题
  - 3.3 -v10打印curl命令
  - 3.4 level的对应关系并做实验修改kubelet的日志级别
  - 3.5 各个组件的configz的逻辑
  - 3.6 apiserver的鉴权基础知识
  - 3.7 分析auth的源码认识RunAccessList流程
  - 3.8 服务端返回SSRR的verb列表逻辑
  - 3.9 cani的单一check
  - 3.10 apiserver的RBAC鉴权

- 第4章 分析一下第1个错误判断函数runtimeError
  - 4.1 回到disk实验的初衷
  - 4.2 查看readySetter中的4个错误判断函数
  - 4.3 分析一下第1个错误判断函数runtimeError
  - 4.4 对比普通pleg和基于event的pleg的准备工作
  - 4.5 安装kube-prometheus监控
  - 4.6 kp套件里面的所有镜像放到统一的仓库里面
  - 4.7 优化并访问kube-prometheus
  - 4.8 分析第1种普通pleg的源码
  - 4.9 分析第2种普通event的源码
  - 4.10 总结2种pleg并追查cri的event

- 第5章 开始查看containerd的代码
  - 5.1 磁盘压力的污点
  - 5.2 分析完第1个cri错误进行notReady实验
  - 5.3 分析一下第2个错误判断函数Network
  - 5.4 containerd中的RuntimeCondition的赋值
  - 5.5 使用grpcurl调用containerd的接口
  - 5.6 学习一下grpcurl
  - 5.7 总结一下
  - 5.8 用go代码调用containerd-grpc接口样例
  - 5.9 测试发现问题并使用crictl
  - 5.10 在grpc调用containerd的时候如何传递namespace变量

- 第6章 开始查看containerd的代码
  - 6.1 代理containerd的unix为tcp
  - 6.2 调用containerd的status接口遇到问题
  - 6.3 切换到上层的cri-api调用
  - 6.4 containerd中的netplugin的init逻辑前序
  - 6.5 学习cni规范01
  - 6.6 学习cni规范中的第2部分-执行协议
  - 6.7 学习cni规范中的第3部分
  - 6.8 containerd中的netplugin的初始化cni的逻辑中的5次执行
  - 6.9 其余init的逻辑
  - 6.10 看下libcni中的addNetwork的主流程

- 第7章 containerd内部调用addNetwork的链条
  - 7.1 addNetwork的create动作
  - 7.2 containerd内部调用addNetwork的链条 
  - 7.3 puml工具简介
  - 7.4 puml工具基础语法
  - 7.5 画出containerd-调用cni的图001
  - 7.6 完成这个puml图
  - 7.7 containerd调用libcni的addNetwork时的传参
  - 7.8 准备实验结合代码看cni支持pod设置带宽
  - 7.9 带宽怎么也打不上去
  - 7.10 进行回顾

- 第8章 pod流量整形
  - 8.1 准备iperf环境进行实验
  - 8.2 实验流量
  - 8.3 流量限制底层是如何做的
  - 8.4 分析calico-cni插件代码
  - 8.5 cri调用各个cni插件的参数和结果
  - 8.6 能否在机器上查看到tc的设置的结果
  - 8.7 cni的插件支持和netns
  - 8.8 带宽限制的puml流程图
  - 8.9 containerd中的netplugin的status逻辑
  - 8.10 完成这个puml图 kubelet的4大ready判断之2的Network底层containerd-status逻辑

- 第9章 cni的ipam分配ip流程查看
  - 9.1 IPAM的准备工作
  - 9.2 实验用IPAM的host-local分配ip
  - 9.3 host-local的代码中的参数解析
  - 9.4 host-local的代码之flock加锁
  - 9.5 host-local的代码之allocate分配
  - 9.6 画一个puml图：cni核心流程ipam之hostLocal类型流程
  - 9.7 官方dhcp流程分析
  - 9.8 学习dhcp流程
  - 9.9 学习calico的架构

- 第10章 calico的ipip模式分析
  - 10.1 ipip模式基础知识通过linkid找网卡抓包
  - 10.2 ipip模式基础知识nsenter进入网络ns
  - 10.3 进行ipip隧道实验
  - 10.4 总结原理
  - 10.5 学习读懂ip-route路由表
  - 10.6 尝试解读master上的main路由表
  - 10.7 尝试解读master上的main路由表-2
  - 10.8 calico-ipip模式本机容器访问的arp代理
  - 10.9 画puml图
  - 10.10 抓calico-ipip模式遇到问题

- 第11章 calico的vxlan模式分析
  - 11.1 解决之前的问题-失败
  - 11.2 解释一下为什么尝试了多种方法都是抓不到ipip包
  - 11.3 画puml图
  - 11.4 准备一个nat模式的vmware虚拟机
  - 11.5 给这个不同网段的节点去安装k8s129版本
  - 11.6 验证一下crossSubNet的vxlan
  - 11.7 尝试添加nat的网卡
  - 11.8 尝试抓crossSubNet的vxlan报文
  - 11.9 calico的模式改变之全vxlan
  - 11.10 calico的模式改变之全IPIP


- TODO: 
  - calico的模式混合使用会通吗
  - IP地址管理插件（IPAM插件）calico的逻辑 存储在哪里，怎么并发的
  - netplugin是不是calico 之间的交互
  - crictl的info 命令是如何调用到containerd的statusv1接口
  - ready到底是谁决定的  4种判断函数任意一种都会导致notReady
- 文章
  - https://blog.51cto.com/u_2010293/2781914
  - https://blog.csdn.net/wanger5354/article/details/122489802

# ready到底是谁决定的  4种判断函数任意一种都会导致notReady
- runtimeErrorsFunc(),
- networkErrorsFunc()
- storageErrorsFunc()
- nodeShutdownManagerErrorsFunc()


# 课程卖点
# 新旧k8s源码解读课程区别

| 课程  | 课程逻辑        | 举例                     | 好处        | 缺点                   |
|-----|-------------|------------------------|-----------|----------------------|
| 旧课程 | 按k8s的组件逐一解读 | apiserver中各个模块的启动到内部逻辑 | 比较全       | 缺乏生产应用问题场景<br/>周期比较长 |
| 新课程 | 从问题入手       |    k8s节点宕机or-kubelet不可用多久后pod会迁移      | 从问题入手便于理解 | 比较零散                 |


# 新课程的流程
> 源码解读新形式: 源码解读和实战问题分析想结合
- 发现问题(经验) --> 复现问题(做实验) --> 分析学习源码(源码) --> 能否二开，写个周边的组件来解决问题(开发)
- 和之前的k8s运维大师很像
  - 运维大师更偏向于 开发组件
  - 源码解读更偏向于 分析源码


> 重点提出问题相关
- 问题：从问题入手
  - 使用k8s过程
- 到查看源码
- 到解决问题的运维开发项目

> 已有的问题列表
- k8s节点宕机or-kubelet不可用多久后pod会迁移
- 关于容器存储的
  - 容器临时文件 容器临时存储监控用量源码追踪和过量使用危害
    - 开发基于kubelet的api 获取容器使用临时存储量
  - 镜像
  - 日志
  - 数据
  - pvc
  - volume
- statefulset index版本号
- containerd根下的run目录满了导致容器创建失败
  - 探针和日志
- k8s源码解读之容器的ready和started标志位


> 基础运作机制
- 镜像分层机制
- 原地升级的机制
- 